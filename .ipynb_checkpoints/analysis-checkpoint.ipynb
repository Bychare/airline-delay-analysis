{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f679f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca1be6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascii\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "with open('2009.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "print(result['encoding'])  # Например, 'Windows-1251'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8471717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paris\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные успешно загружены\n",
      "Размер данных: (1170501, 8)\n",
      "\n",
      "Данные готовы для машинного обучения:\n",
      "- Обучающая выборка: (936400, 7)\n",
      "- Тестовая выборка: (234101, 7)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def try_spark_loading(file_path):\n",
    "    \"\"\"Пытается загрузить данные через Spark, если он доступен\"\"\"\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark.sql.functions import col\n",
    "        \n",
    "        # Инициализация Spark\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"Airplane Delay Prediction\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        # Загрузка данных\n",
    "        df = spark.read.csv(\n",
    "            file_path,\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    "            mode=\"PERMISSIVE\"\n",
    "        )\n",
    "        \n",
    "        # Выбор нужных колонок и преобразование в Pandas\n",
    "        selected_cols = [\n",
    "            'DEP_DELAY', 'DISTANCE', 'CRS_ELAPSED_TIME', 'CARRIER_DELAY',\n",
    "            'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY'\n",
    "        ]\n",
    "        pandas_df = df.select(selected_cols).dropna().toPandas()\n",
    "        \n",
    "        spark.stop()\n",
    "        return pandas_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Не удалось загрузить через Spark, пробуем Pandas. Ошибка: {e}\")\n",
    "        return None\n",
    "\n",
    "def pandas_loading(file_path):\n",
    "    \"\"\"Загружает данные напрямую через Pandas\"\"\"\n",
    "    try:\n",
    "        # Указываем только нужные колонки для экономии памяти\n",
    "        usecols = [\n",
    "            'DEP_DELAY', 'DISTANCE', 'CRS_ELAPSED_TIME', 'CARRIER_DELAY',\n",
    "            'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY'\n",
    "        ]\n",
    "        \n",
    "        # Чтение файла с обработкой ошибок\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            usecols=usecols,\n",
    "            low_memory=False,\n",
    "            encoding='ISO-8859-1',\n",
    "            on_bad_lines='skip'\n",
    "        )\n",
    "        \n",
    "        # Удаление строк с пропущенными значениями\n",
    "        df = df.dropna()\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке через Pandas: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Основная функция загрузки данных\"\"\"\n",
    "    # Сначала пробуем Spark\n",
    "    pandas_df = try_spark_loading(file_path)\n",
    "    \n",
    "    # Если не получилось, используем Pandas\n",
    "    if pandas_df is None:\n",
    "        print(\"Используем Pandas для загрузки данных\")\n",
    "        pandas_df = pandas_loading(file_path)\n",
    "    \n",
    "    print(\"Данные успешно загружены\")\n",
    "    print(f\"Размер данных: {pandas_df.shape}\")\n",
    "    return pandas_df\n",
    "\n",
    "def prepare_for_ml(pandas_df):\n",
    "    \"\"\"Подготавливает данные для машинного обучения\"\"\"\n",
    "    try:\n",
    "        X = pandas_df.drop(columns=['DEP_DELAY'])\n",
    "        y = pandas_df['DEP_DELAY']\n",
    "        \n",
    "        # Разделение данных\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=0.2, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Масштабирование\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        print(\"\\nДанные готовы для машинного обучения:\")\n",
    "        print(f\"- Обучающая выборка: {X_train_scaled.shape}\")\n",
    "        print(f\"- Тестовая выборка: {X_test_scaled.shape}\")\n",
    "        \n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при подготовке для ML: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Основной поток выполнения\"\"\"\n",
    "    try:\n",
    "        # Путь к файлу данных\n",
    "        file_path = \"2009.csv\"  # Убедитесь, что файл в той же директории\n",
    "        \n",
    "        # 1. Загрузка данных\n",
    "        pandas_df = load_data(file_path)\n",
    "        \n",
    "        # 2. Подготовка для ML\n",
    "        X_train, X_test, y_train, y_test, scaler = prepare_for_ml(pandas_df)\n",
    "        \n",
    "        # Здесь можно продолжить с построением моделей...\n",
    "        return X_train, X_test, y_train, y_test, scaler\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Критическая ошибка: {e}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_train, X_test, y_train, y_test, scaler = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afcd5b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных...\n",
      "Данные успешно загружены. Размер: (1170501, 8)\n",
      "\n",
      "Подготовка данных...\n",
      "Данные готовы для ML:\n",
      "- Обучающая выборка: (936400, 7)\n",
      "- Тестовая выборка: (234101, 7)\n",
      "\n",
      "Обучение моделей...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обучение моделей:  44%|█████████████████████████████▎                                    | 4/9 [00:43<01:24, 16.96s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Конфигурация\n",
    "CONFIG = {\n",
    "    \"data\": {\n",
    "        \"file_path\": \"2009.csv\",\n",
    "        \"features\": [\n",
    "            'DEP_DELAY', 'DISTANCE', 'CRS_ELAPSED_TIME', 'CARRIER_DELAY',\n",
    "            'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY'\n",
    "        ],\n",
    "        \"target\": \"DEP_DELAY\",\n",
    "        \"output_dir\": \"output\"\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"Ridge\": {\"alpha\": 1.0},\n",
    "        \"Lasso\": {\"alpha\": 0.1},\n",
    "        \"ElasticNet\": {\"alpha\": 0.1, \"l1_ratio\": 0.7},\n",
    "        \"RandomForest\": {\"n_estimators\": 100, \"max_depth\": 10, \"random_state\": 42, \"n_jobs\": -1},\n",
    "        \"GradientBoosting\": {\"n_estimators\": 100, \"learning_rate\": 0.1, \"random_state\": 42},\n",
    "        \"XGBoost\": {\"n_estimators\": 100, \"learning_rate\": 0.1, \"random_state\": 42},\n",
    "        \"CatBoost\": {\"iterations\": 100, \"learning_rate\": 0.1, \"verbose\": 0, \"random_state\": 42},\n",
    "        \"LightGBM\": {\"n_estimators\": 100, \"learning_rate\": 0.1, \"random_state\": 42},\n",
    "        \"kNN\": {\"n_neighbors\": 5, \"n_jobs\": -1}\n",
    "    },\n",
    "    \"test_size\": 0.2,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Загружает данные через Pandas\"\"\"\n",
    "    try:\n",
    "        print(\"Загрузка данных...\")\n",
    "        df = pd.read_csv(\n",
    "            CONFIG[\"data\"][\"file_path\"],\n",
    "            usecols=CONFIG[\"data\"][\"features\"],\n",
    "            low_memory=False,\n",
    "            encoding='ISO-8859-1',\n",
    "            on_bad_lines='skip'\n",
    "        )\n",
    "        \n",
    "        # Удаление строк с пропущенными значениями\n",
    "        df = df.dropna()\n",
    "        print(f\"Данные успешно загружены. Размер: {df.shape}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке данных: {e}\")\n",
    "        raise\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"Подготавливает данные для машинного обучения\"\"\"\n",
    "    print(\"\\nПодготовка данных...\")\n",
    "    X = df.drop(columns=[CONFIG[\"data\"][\"target\"]])\n",
    "    y = df[CONFIG[\"data\"][\"target\"]]\n",
    "    \n",
    "    # Разделение данных\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=CONFIG[\"test_size\"], \n",
    "        random_state=CONFIG[\"random_state\"]\n",
    "    )\n",
    "    \n",
    "    # Масштабирование\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"Данные готовы для ML:\")\n",
    "    print(f\"- Обучающая выборка: {X_train_scaled.shape}\")\n",
    "    print(f\"- Тестовая выборка: {X_test_scaled.shape}\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "def train_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Обучает и оценивает модели\"\"\"\n",
    "    print(\"\\nОбучение моделей...\")\n",
    "    \n",
    "    models = {\n",
    "        'Ridge': Ridge(**CONFIG[\"models\"][\"Ridge\"]),\n",
    "        'Lasso': Lasso(**CONFIG[\"models\"][\"Lasso\"]),\n",
    "        'ElasticNet': ElasticNet(**CONFIG[\"models\"][\"ElasticNet\"]),\n",
    "        'RandomForest': RandomForestRegressor(**CONFIG[\"models\"][\"RandomForest\"]),\n",
    "        'GradientBoosting': GradientBoostingRegressor(**CONFIG[\"models\"][\"GradientBoosting\"]),\n",
    "        'XGBoost': XGBRegressor(**CONFIG[\"models\"][\"XGBoost\"]),\n",
    "        'CatBoost': CatBoostRegressor(**CONFIG[\"models\"][\"CatBoost\"]),\n",
    "        'LightGBM': LGBMRegressor(**CONFIG[\"models\"][\"LightGBM\"]),\n",
    "        'kNN': KNeighborsRegressor(**CONFIG[\"models\"][\"kNN\"])\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in tqdm(models.items(), desc=\"Обучение моделей\"):\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "            preds = model.predict(X_test)\n",
    "            \n",
    "            results[name] = {\n",
    "                'MSE': mean_squared_error(y_test, preds),\n",
    "                'MAE': mean_absolute_error(y_test, preds),\n",
    "                'R2': r2_score(y_test, preds),\n",
    "                'model': model\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка с моделью {name}: {e}\")\n",
    "            results[name] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_results(results, y_test, best_model_preds):\n",
    "    \"\"\"Создает визуализации результатов\"\"\"\n",
    "    print(\"\\nСоздание визуализаций...\")\n",
    "    os.makedirs(CONFIG[\"data\"][\"output_dir\"], exist_ok=True)\n",
    "    \n",
    "    # 1. Сравнение моделей по MSE\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    mse_values = [v['MSE'] for v in results.values() if v is not None]\n",
    "    model_names = [k for k, v in results.items() if v is not None]\n",
    "    sns.barplot(x=model_names, y=mse_values, palette='viridis')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.title(\"Сравнение моделей по MSE\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG[\"data\"][\"output_dir\"], \"model_comparison.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Лучшая модель: фактические vs предсказанные значения\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, best_model_preds, alpha=0.3)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel(\"Фактическая задержка\")\n",
    "    plt.ylabel(\"Предсказанная задержка\")\n",
    "    plt.title(\"Фактические vs предсказанные значения задержки\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG[\"data\"][\"output_dir\"], \"actual_vs_predicted.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Распределение ошибок\n",
    "    errors = y_test - best_model_preds\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(errors, bins=100, kde=True)\n",
    "    plt.xlabel(\"Ошибка предсказания\")\n",
    "    plt.title(\"Распределение ошибок предсказания\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG[\"data\"][\"output_dir\"], \"error_distribution.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def save_best_model(results):\n",
    "    \"\"\"Сохраняет лучшую модель\"\"\"\n",
    "    valid_results = {k: v for k, v in results.items() if v is not None}\n",
    "    if not valid_results:\n",
    "        print(\"Нет валидных моделей для сохранения\")\n",
    "        return\n",
    "    \n",
    "    best_model_name = min(valid_results, key=lambda x: valid_results[x]['MSE'])\n",
    "    best_model = valid_results[best_model_name]['model']\n",
    "    \n",
    "    model_path = os.path.join(CONFIG[\"data\"][\"output_dir\"], \"best_model.pkl\")\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"\\nЛучшая модель ({best_model_name}) сохранена в {model_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Основной поток выполнения\"\"\"\n",
    "    try:\n",
    "        # 1. Загрузка данных\n",
    "        df = load_data()\n",
    "        \n",
    "        # 2. Подготовка данных\n",
    "        X_train, X_test, y_train, y_test, _ = prepare_data(df)\n",
    "        \n",
    "        # 3. Обучение моделей\n",
    "        results = train_models(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # 4. Определение лучшей модели\n",
    "        valid_results = {k: v for k, v in results.items() if v is not None}\n",
    "        best_model_name = min(valid_results, key=lambda x: valid_results[x]['MSE'])\n",
    "        best_model_preds = valid_results[best_model_name]['model'].predict(X_test)\n",
    "        \n",
    "        # 5. Визуализация\n",
    "        visualize_results(results, y_test, best_model_preds)\n",
    "        \n",
    "        # 6. Сохранение лучшей модели\n",
    "        save_best_model(results)\n",
    "        \n",
    "        print(\"\\nАнализ успешно завершен!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nОшибка в основном потоке выполнения: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Путь к папке с ноутбуком\n",
    "folder_path = r'C:\\Users\\paris\\Desktop\\Untitled Folder'\n",
    "kaggle_json_path = os.path.join(folder_path, 'kaggle.json')\n",
    "\n",
    "# Читаем учетные данные из kaggle.json\n",
    "with open(kaggle_json_path, 'r') as f:\n",
    "    kaggle_creds = json.load(f)\n",
    "\n",
    "# Устанавливаем переменные окружения\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_creds['username']\n",
    "os.environ['KAGGLE_KEY'] = kaggle_creds['key']\n",
    "\n",
    "# Инициализируем API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Скачиваем датасет (архив)\n",
    "dataset_name = 'yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018'\n",
    "zip_path = os.path.join(folder_path, 'airline-delay-and-cancellation-data-2009-2018.zip')\n",
    "\n",
    "# Скачиваем весь архив (так надежнее)\n",
    "api.dataset_download_files(\n",
    "    dataset=dataset_name,\n",
    "    path=folder_path,\n",
    "    force=True,\n",
    "    quiet=False,\n",
    "    unzip=False  # Отключаем автоматическую распаковку\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331af54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Распаковываем только нужный файл\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    # Ищем полное имя файла в архиве (может быть с подпапками)\n",
    "    target_file = next(f for f in zip_ref.namelist() if f.endswith('2009.csv'))\n",
    "    \n",
    "    # Извлекаем файл\n",
    "    zip_ref.extract(target_file, folder_path)\n",
    "    \n",
    "    # Если файл был во вложенной папке, перемещаем его\n",
    "    extracted_path = os.path.join(folder_path, target_file)\n",
    "    final_path = os.path.join(folder_path, '2009.csv')\n",
    "    \n",
    "    if extracted_path != final_path:\n",
    "        os.rename(extracted_path, final_path)\n",
    "        # Удаляем пустые папки, если они остались\n",
    "        try:\n",
    "            os.removedirs(os.path.dirname(extracted_path))\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "# Удаляем архив (раскомментируйте если нужно)\n",
    "os.remove(zip_path)\n",
    "\n",
    "print(f\"Файл успешно сохранен: {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d156ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd97eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
